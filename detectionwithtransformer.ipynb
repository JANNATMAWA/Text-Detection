{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8673877,"sourceType":"datasetVersion","datasetId":5198837}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-18T20:54:51.674378Z","iopub.execute_input":"2024-06-18T20:54:51.674670Z","iopub.status.idle":"2024-06-18T20:54:52.986604Z","shell.execute_reply.started":"2024-06-18T20:54:51.674645Z","shell.execute_reply":"2024-06-18T20:54:52.985561Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -i https://test.pypi.org/simple/ supervision==0.3.0\n!pip install -q transformers\n!pip install -q pytorch-lightning\n!pip install -q timm\n!pip install -q roboflow\n!pip install -q pycocotools\n!pip install wurlitzer\n!pip install jupyter-lsp==2.0.0\n!pip install packaging>=22\n!pip install shapely>=2.0.1\n!pip install keras==2.15.0\n!pip install numpy<1.26\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import DetrForObjectDetection, DetrImageProcessor\nimport supervision as sv\nimport pytorch_lightning as pl\n\n# settings\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nCHECKPOINT = 'facebook/detr-resnet-50'\nCONFIDENCE_THRESHOLD = 0.5\nIOU_THRESHOLD = 0.8\n\nimage_processor = DetrImageProcessor.from_pretrained(CHECKPOINT)\nmodel = DetrForObjectDetection.from_pretrained(CHECKPOINT)\nmodel.to(DEVICE)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\n\ndef get_id2label(labels_folder):\n    class_ids = set()\n    label_paths = glob.glob(os.path.join(labels_folder, \"*.txt\"))\n    for label_path in label_paths:\n        with open(label_path, 'r') as f:\n            for line in f:\n                class_id = int(line.strip().split()[0])\n                class_ids.add(class_id)\n    id2label = {class_id: \"text_region\" for class_id in class_ids}\n    return id2label\n\n# Define id2label for your dataset\nid2label = get_id2label(\"/kaggle/input/yolovmawa/train-output-labels\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport glob\nimport cv2\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class YOLOv8Dataset(Dataset):\n    def __init__(self, images_folder, labels_folder, image_processor, id2label):\n        self.images_folder = images_folder\n        self.labels_folder = labels_folder\n        self.image_processor = image_processor\n        self.id2label = id2label\n        self.image_paths = glob.glob(os.path.join(images_folder, \"*.jpg\"))\n        self.label_paths = glob.glob(os.path.join(labels_folder, \"*.txt\"))\n        self.transforms = T.Compose([\n            T.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        label_path = os.path.join(self.labels_folder, os.path.basename(image_path).replace(\".jpg\", \".txt\"))\n\n        # Load image\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = self.transforms(image)\n        image = image / 255.0  # Ensure image values are within [0, 1]\n\n        # Load annotations\n        boxes = []\n        labels = []\n        areas = []\n        with open(label_path, 'r') as f:\n            for line in f:\n                class_id, x_center, y_center, width, height = map(float, line.strip().split())\n                labels.append(int(class_id))\n                x_center *= image.shape[2]\n                y_center *= image.shape[1]\n                width *= image.shape[2]\n                height *= image.shape[1]\n                x_min = x_center - width / 2\n                y_min = y_center - height / 2\n                boxes.append([x_min, y_min, width, height])\n                areas.append(width * height)  # Calculate area\n\n        # Prepare annotations in COCO format\n        annotations = {\n            \"image_id\": idx,\n            \"annotations\": [\n                {\"bbox\": box, \"category_id\": label, \"area\": area}\n                for box, label, area in zip(boxes, labels, areas)\n            ]\n        }\n\n        encoding = self.image_processor(images=image, annotations=annotations, return_tensors=\"pt\", do_rescale=False, size={\"shortest_edge\": 800, \"longest_edge\": 800})\n        pixel_values = encoding[\"pixel_values\"].squeeze()\n        target = encoding[\"labels\"][0]\n\n        return pixel_values, target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n    pixel_values = [item[0] for item in batch]\n    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n    labels = [item[1] for item in batch]\n    return {\n        'pixel_values': encoding['pixel_values'],\n        'pixel_mask': encoding['pixel_mask'],\n        'labels': labels\n    }\n\ntrain_dataset = YOLOv8Dataset(images_folder=\"/kaggle/input/yolovmawa/train1\", labels_folder=\"/kaggle/input/yolovmawa/train-output-labels\", image_processor=image_processor, id2label=id2label)\nval_dataset = YOLOv8Dataset(images_folder=\"/kaggle/input/yolovmawa/valid1\", labels_folder=\"/kaggle/input/yolovmawa/output-labels\", image_processor=image_processor, id2label=id2label)\ntest_dataset = YOLOv8Dataset(images_folder=\"/kaggle/input/yolovmawa/test1\", labels_folder=\"/kaggle/input/yolovmawa/testpost/output\", image_processor=image_processor, id2label=id2label)\n\ntrain_dataloader = DataLoader(dataset=train_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True, num_workers=3)\nval_dataloader = DataLoader(dataset=val_dataset, collate_fn=collate_fn, batch_size=4, num_workers=3)\ntest_dataloader = DataLoader(dataset=test_dataset, collate_fn=collate_fn, batch_size=4, num_workers=3)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Detr(pl.LightningModule):\n    def __init__(self, lr, lr_backbone, weight_decay):\n        super().__init__()\n        self.model = DetrForObjectDetection.from_pretrained(\n            pretrained_model_name_or_path=CHECKPOINT, \n            num_labels=len(id2label),\n            ignore_mismatched_sizes=True\n        )\n        self.lr = lr\n        self.lr_backbone = lr_backbone\n        self.weight_decay = weight_decay\n\n    def forward(self, pixel_values, pixel_mask):\n        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n\n    def common_step(self, batch, batch_idx):\n        pixel_values = batch[\"pixel_values\"]\n        pixel_mask = batch[\"pixel_mask\"]\n        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n\n        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n        loss = outputs.loss\n        loss_dict = outputs.loss_dict\n        return loss, loss_dict\n\n    def training_step(self, batch, batch_idx):\n        loss, loss_dict = self.common_step(batch, batch_idx)     \n        self.log(\"training_loss\", loss)\n        for k,v in loss_dict.items():\n            self.log(\"train_\" + k, v.item())\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss, loss_dict = self.common_step(batch, batch_idx)     \n        self.log(\"validation/loss\", loss)\n        for k, v in loss_dict.items():\n            self.log(\"validation_\" + k, v.item())\n        return loss\n\n    def configure_optimizers(self):\n        param_dicts = [\n            {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n            {\"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad], \"lr\": self.lr_backbone},\n        ]\n        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n\nmodel = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n\nfrom pytorch_lightning import Trainer\n\ntrainer = Trainer(devices=1, accelerator=\"gpu\", max_epochs=60, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\ntrainer.fit(model, train_dataloader, val_dataloader)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Check if the test images directory exists and list the files\ntest_image_dir = \"/kaggle/input/yolovmawa/test1\"\nif not os.path.exists(test_image_dir):\n    print(f\"Directory {test_image_dir} does not exist.\")\nelse:\n    test_image_paths = glob.glob(os.path.join(test_image_dir, \"*.jpg\"))\n    if not test_image_paths:\n        print(f\"No images found in {test_image_dir} directory.\")\n    else:\n        print(f\"Found {len(test_image_paths)} images in the test directory.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\ndef calculate_metrics(predictions, ground_truths, iou_threshold=0.5):\n    tp = 0\n    fp = 0\n    fn = 0\n\n    for prediction, ground_truth in zip(predictions, ground_truths):\n        pred_boxes = prediction['boxes']\n        pred_labels = prediction['labels']\n        pred_scores = prediction['scores']\n\n        gt_boxes = ground_truth['boxes']\n        gt_labels = ground_truth['labels']\n\n        # Match predictions with ground truth boxes\n        for pred_box, pred_label in zip(pred_boxes, pred_labels):\n            max_iou = 0\n            matched_gt = None\n\n            for gt_box, gt_label in zip(gt_boxes, gt_labels):\n                if pred_label == gt_label:\n                    iou = compute_iou(pred_box, gt_box)\n                    if iou > max_iou:\n                        max_iou = iou\n                        matched_gt = gt_box\n\n            if max_iou >= iou_threshold:\n                tp += 1\n                gt_boxes.remove(matched_gt)\n            else:\n                fp += 1\n\n        fn += len(gt_boxes)\n\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n\n    return precision, recall, f1\n\ndef compute_iou(box1, box2):\n    x1, y1, w1, h1 = box1\n    x2, y2, w2, h2 = box2\n\n    xi1 = max(x1, x2)\n    yi1 = max(y1, y2)\n    xi2 = min(x1 + w1, x2 + w2)\n    yi2 = min(y1 + h1, y2 + h2)\n\n    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n    box1_area = w1 * h1\n    box2_area = w2 * h2\n\n    union_area = box1_area + box2_area - inter_area\n    iou = inter_area / union_area if union_area != 0 else 0\n\n    return iou\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import precision_recall_fscore_support\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCONFIDENCE_THRESHOLD = 0.5\n\ndef calculate_metrics(y_true, y_pred):\n    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n    return precision, recall, f1\n\ndef evaluate_custom(test_dataloader, model, image_processor):\n    all_precisions = []\n    all_recalls = []\n    all_f1s = []\n\n    # Move model to the device (GPU or CPU)\n    model.to(DEVICE)\n    \n    for idx, batch in enumerate(tqdm(test_dataloader)):\n        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n        pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\n        labels = [{k: v.to(DEVICE) for k, v in t.items()} for t in batch[\"labels\"]]\n\n        with torch.no_grad():\n            outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n\n        orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n        results = image_processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes)\n\n        for target, result in zip(labels, results):\n            prediction = {\n                \"boxes\": result[\"boxes\"].cpu().numpy(),\n                \"labels\": result[\"labels\"].cpu().numpy(),\n                \"scores\": result[\"scores\"].cpu().numpy()\n            }\n            ground_truth = {\n                \"boxes\": target[\"boxes\"].cpu().numpy(),\n                \"labels\": target[\"class_labels\"].cpu().numpy()  # Updated key for labels\n            }\n\n            if len(prediction[\"labels\"]) > 0:  # Ensure there are predictions\n                precision, recall, f1 = calculate_metrics(ground_truth[\"labels\"], prediction[\"labels\"])\n                all_precisions.append(precision)\n                all_recalls.append(recall)\n                all_f1s.append(f1)\n            else:\n                print(f\"No predictions for image {idx}\")\n\n    # Compute average metrics\n    if all_precisions and all_recalls and all_f1s:\n        avg_precision = sum(all_precisions) / len(all_precisions)\n        avg_recall = sum(all_recalls) / len(all_recalls)\n        avg_f1 = sum(all_f1s) / len(all_f1s)\n\n        print(f\"Average Precision: {avg_precision:.4f}, Average Recall: {avg_recall:.4f}, Average F1-score: {avg_f1:.4f}\")\n    else:\n        print(\"No valid predictions to calculate metrics.\")\n\n# Ensure the model and dataloader are on the same device\nmodel.to(DEVICE)\n\n# Evaluate the model on the test dataset\nevaluate_custom(test_dataloader, model, image_processor)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}